{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import sys\n",
    "sys.path.append('/src/')\n",
    "from visualization.visualize import print_image_with_point, show_complete_fixation_with_all_frames_all_gaze\n",
    "\n",
    "# use defaul plot style\n",
    "plt.style.use('default')\n",
    "WIDTH = 6\n",
    "dpi = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"path/to/folder/with/data/\"\n",
    "\n",
    "# evaluation data\n",
    "eval_path = folder_path + \"data/cyprus_eval_frames.csv\"\n",
    "eval_path_results = folder_path + \"data/cyprus_eval_frames_results.csv\"\n",
    "cyprus_eval_frames_results_predictions = folder_path + \"data/cyprus_eval_frames_results_predictions.csv\"\n",
    "cyprus_eval_complete_sumpXc = folder_path + \"data/cyprus_eval_complete_sumpXc.csv\"\n",
    "\n",
    "# load the names and print the categories\n",
    "p = folder_path + \"label_mapping.json\"\n",
    "with open(p, 'r') as f:\n",
    "    label_mapping = json.load(f)\n",
    "    categories = set(label_mapping[\"category_mapping\"].values())\n",
    "# note :   \n",
    "# cma active agent\n",
    "# sa passive agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df_res_pred = pd.read_csv(cyprus_eval_frames_results_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the columns\n",
    "df_res_pred[\"manual_label\"] = df_res_pred['response']\n",
    "df_res_pred.drop(columns=[\"response\"], inplace=True)\n",
    "\n",
    "# rename the 4Level to ambiguous\n",
    "df_res_pred.loc[df_res_pred[\"level_sum\"]==4, \"predicted_sum\"] = \"ambiguous\"  \n",
    "df_res_pred.loc[df_res_pred[\"level_pXc\"]==4, \"predicted_pXc\"] = \"ambiguous\" \n",
    "\n",
    "# check if response was correct for the two differenv ways of calulating the score\n",
    "df_res_pred[\"correct_sum\"] = df_res_pred[\"manual_label\"] == df_res_pred[\"predicted_sum\"] \n",
    "df_res_pred[\"correct_pXc\"] = df_res_pred[\"manual_label\"] == df_res_pred[\"predicted_pXc\"] \n",
    "  \n",
    "# show the tabel\n",
    "df_res_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### analysis of the results taking out the ambiguous results\n",
    "df_res_pred_clean = df_res_pred[df_res_pred[ \"manual_label\"] != \"ambiguous\"]\n",
    "df_res_pred_amb = df_res_pred[df_res_pred[ \"manual_label\"] == \"ambiguous\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_res_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"correct_pXc\"].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct counts per level\n",
    "# to datafram\n",
    "df_level = df.groupby(\"level_pXc\")[\"correct_pXc\"].value_counts().unstack().fillna(0).astype(int).reset_index()\n",
    "# delete index and set the level as index\n",
    "df_level = df_level.set_index(\"level_pXc\", inplace=True)\n",
    "df_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results per level as a stacked bar plot\n",
    "df_level.plot(kind='bar', stacked=True, color = [\"red\", \"green\"], figsize=(WIDTH*1.3, WIDTH), dpi=dpi )\n",
    "plt.xticks(rotation=0)\n",
    "plt.xlabel(\"Prediction Level\")\n",
    "plt.ylabel(\"Fixation Count\")\n",
    "plt.legend([\"Incorrect\", \"Correct\"])\n",
    "plt.xticks(np.arange(0, 4), [ \"1\", \"2\", \"3\", \"4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcultate recall and precision for each category\n",
    "\n",
    "# modify below -----------------------------------------------------------\n",
    "heuristic = \"predicted_pXc\"  #'predicted_sum'#\n",
    "\n",
    "#df = df[df.level_pXc == 3] # chose if it shoulb be just for a specific level\n",
    "# modify above -----------------------------------------------------------\n",
    "\n",
    "categorie_list = sorted(list(categories) + [\"ambiguous\"])\n",
    "recall_dict = {}\n",
    "precision_dict = {}\n",
    "accuracy_dict = {}\n",
    "\n",
    "for category in categorie_list:\n",
    "    TP = len(df[(df[\"manual_label\"] == category) & (df[heuristic] == category)])\n",
    "    FP = len(df[(df[\"manual_label\"] != category) & (df[heuristic] == category)])\n",
    "    TN = len(df[(df[\"manual_label\"] != category) & (df[heuristic] != category)])\n",
    "    FN = len(df[(df[\"manual_label\"] == category) & (df[heuristic] != category)])\n",
    "\n",
    "    recall_dict[category] = (\n",
    "        TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    )  # how good can the model find all correct ones\n",
    "    precision_dict[category] = (\n",
    "        TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    )  # how good can the model find only correct ones\n",
    "    accuracy_dict[category] = (\n",
    "        (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "    )\n",
    "\n",
    "df_per_cat = {\n",
    "    \"category\": list(recall_dict.keys()),\n",
    "    \"recall\": list(recall_dict.values()),\n",
    "    \"precision\": list(precision_dict.values()),\n",
    "    \"accuracy\": list(accuracy_dict.values()),\n",
    "    \"ammount\": list(\n",
    "        df_res_pred[\"manual_label\"]\n",
    "        .value_counts()\n",
    "        .reindex(categorie_list)\n",
    "        .fillna(0)\n",
    "        .astype(int)\n",
    "    ),\n",
    "}\n",
    "# create df_per_cat\n",
    "df_per_cat = pd.DataFrame(df_per_cat)\n",
    "df_per_cat = df_per_cat[df_per_cat[\"ammount\"] != 0].reset_index(drop=True)\n",
    "df_per_cat[\"weighted_recall\"] = df_per_cat[\"recall\"] * df_per_cat[\"ammount\"]\n",
    "df_per_cat[\"weigth\"] = df_per_cat[\"ammount\"] / df_per_cat[\"ammount\"].sum()\n",
    "df_per_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig, ax = plt.subplots(figsize=(WIDTH*1.3, WIDTH), dpi=dpi) \n",
    "width = 0.35\n",
    "\n",
    "recall_bars = ax.bar(np.arange(len(categorie_list)), recall_dict.values(), width, label='Recall', color=plt.cm.Blues(700))\n",
    "precision_bars = ax.bar(np.arange(len(categorie_list)) + width, precision_dict.values(), width, label='Precision', color=plt.cm.Blues(100))\n",
    "\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Recall and Precision ')\n",
    "ax.set_xticks(np.arange(len(categorie_list)) + width / 2)\n",
    "ax.set_xticklabels(categorie_list, rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "def autolabel(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    " \n",
    "# just to read the values        \n",
    "#autolabel(recall_bars)\n",
    "#autolabel(precision_bars)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, average_precision_score \n",
    "heuristic = 'predicted_pXc'#'predicted_sum'# \n",
    "\n",
    "print(f'mean accuracy: {sum(df_per_cat[\"accuracy\"]) / len(df_per_cat):.3f}')\n",
    "print(f'Weighted mean accuracy: {sum(df_per_cat[\"accuracy\"]* df_per_cat[\"ammount\"]) / sum(df_per_cat[\"ammount\"]):.3f}')\n",
    "\n",
    "print(f'my Balanced recall: {sum(df_per_cat[\"recall\"]) / len(df_per_cat):.3f}')\n",
    "print(f'my Weighted balanced recall: {sum(df_per_cat[\"weighted_recall\"]) / sum(df_per_cat[\"ammount\"]):.3f}')\n",
    "\n",
    "print(f'my Balanced precision: {sum(df_per_cat[\"precision\"]) / len(df_per_cat):.3f}')\n",
    "print(f'my Weighted balanced precision: {sum(df_per_cat[\"precision\"]* df_per_cat[\"ammount\"]) / sum(df_per_cat[\"ammount\"]):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the category names\n",
    "category_names = ['ambiguous', 'building', 'person', 'signs', 'street', 'vegetation', 'vehicle']\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(df['manual_label'], df[heuristic], labels=category_names)\n",
    "\n",
    "# Normalize the confusion matrix to get percentages\n",
    "cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(WIDTH*1.3, WIDTH), dpi=dpi)\n",
    "im = ax.imshow(cm_percentage, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "# Set the category names as x and y axis labels\n",
    "ax.set(xticks=np.arange(cm.shape[1]),\n",
    "       yticks=np.arange(cm.shape[0]),\n",
    "       xticklabels=category_names, yticklabels=category_names,\n",
    "       xlabel='Predicted label', ylabel='True label')\n",
    "\n",
    "# Rotate the x-axis labels for better readability\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over the data and create text annotations for each cell\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, format(cm_percentage[i, j], '.2f'),\n",
    "                ha=\"center\", va=\"center\", color=\"white\" if cm_percentage[i, j] > 0.5 else \"black\")\n",
    "\n",
    "# add title \n",
    "ax.set_xlabel(\"Predicted Category\")\n",
    "ax.set_ylabel(\"True Category\")\n",
    "# Show the plot\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some basic statistics\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, average_precision_score \n",
    "heuristic = 'predicted_pXc'#'predicted_sum'# \n",
    "\n",
    "df_cleaned = df#[df[\"manual_label\"] != \"ambiguous\"]\n",
    "#df_cleaned = df_cleaned[df_cleaned[\"manual_label\"] != \"sign\"]\n",
    "\n",
    "print(f'mean accuracy: {sum(df_per_cat[\"accuracy\"]) / len(df_per_cat):.3f}')\n",
    "print(f'Weighted mean accuracy: {sum(df_per_cat[\"accuracy\"]* df_per_cat[\"ammount\"]) / sum(df_per_cat[\"ammount\"]):.3f}')\n",
    "\n",
    "print(f'my Balanced recall: {sum(df_per_cat[\"recall\"]) / len(df_per_cat):.3f}')\n",
    "print(f'my Weighted balanced recall: {sum(df_per_cat[\"weighted_recall\"]) / sum(df_per_cat[\"ammount\"]):.3f}')\n",
    "\n",
    "print(f'my Balanced precision: {sum(df_per_cat[\"precision\"]) / len(df_per_cat):.3f}')\n",
    "print(f'my Weighted balanced precision: {sum(df_per_cat[\"precision\"]* df_per_cat[\"ammount\"]) / sum(df_per_cat[\"ammount\"]):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \n",
    "# Adjusting annotations to minimize overlay\n",
    "# Manually adjust the positions of the annotations for the overlapping categories\n",
    "\n",
    "# Create the plot again\n",
    "fig, ax = plt.subplots(figsize=(WIDTH*1.3, WIDTH), dpi=dpi)\n",
    "scatter = ax.scatter(df_per_cat[\"recall\"], df_per_cat[\"precision\"], marker = \"x\", s=[a*5 for a in df_per_cat[\"ammount\"]], color=plt.cm.Blues(180))\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "#ax.set_title('Recall vs Precision')\n",
    "ax.set_xlim(0, 1.1)\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "# Adjusted annotations\n",
    "adjusted_positions = {\n",
    "    'vehicle': (0, -15)  # Shift up\n",
    "    #'person': (10, 5)    # Shift down\n",
    "}\n",
    "\n",
    "#Annotate the dots with adjusted positions for \"building\" and \"person\"\n",
    "for i, category in enumerate(df_per_cat[\"category\"]):\n",
    "    offset = adjusted_positions.get(category, (0, 10))  # Default offset if not in adjusted_positions\n",
    "    ax.annotate(category, (df_per_cat[\"recall\"][i], df_per_cat[\"precision\"][i]), textcoords=\"offset points\", xytext=offset, ha='center')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ba-cv-sam-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
