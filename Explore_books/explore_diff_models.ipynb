{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Semantic Segmentation Models\n",
    "- in this book I test some semantic segmentatin model. It is best run on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/src/\")\n",
    "\n",
    "from visualization.visualize import get_seg_img, get_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    Mask2FormerForUniversalSegmentation,\n",
    "    ViTModel,\n",
    "    BeitFeatureExtractor,\n",
    ")\n",
    "from transformers import (\n",
    "    BeitForSemanticSegmentation,\n",
    "    AutoProcessor,\n",
    "    CLIPSegForImageSegmentation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"\"  # path to the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to test how one model worsks\n",
    "i = 1\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "print(model_name)\n",
    "# Load the model and its preprocessor\n",
    "processor = SegformerImageProcessor.from_pretrained(model_name)\n",
    "model = ViTModel.from_pretrained(model_name)\n",
    "\n",
    "image = Image.open(image_path)  # Load an image\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "logits = outputs.last_hidden_state\n",
    "\n",
    "upsampled_logits = torch.nn.functional.interpolate(\n",
    "    logits,\n",
    "    size=image.size[::-1],  # (height, width)\n",
    "    mode=\"bilinear\",\n",
    "    align_corners=False,\n",
    ")\n",
    "\n",
    "img_seg, seg = get_seg_img(\n",
    "    image, upsampled_logits.argmax(dim=1)[0]\n",
    ")  # Assuming get_seg_img is defined\n",
    "plt.imshow(seg)\n",
    "plt.axis(\"off\")  # Optional: to hide axes for a cleaner visualization\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSemanticSegmentation, AutoFeatureExtractor\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Define the model names\n",
    "model_names = [\n",
    "    \"nvidia/segformer-b3-finetuned-ade-512-512\",\n",
    "    \"nvidia/segformer-b1-finetuned-cityscapes-1024-1024\",\n",
    "    \"nvidia/segformer-b3-finetuned-cityscapes-1024-1024\",\n",
    "    \"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\",\n",
    "]\n",
    "\n",
    "logits_list = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(model_name)\n",
    "    # Load the model and its preprocessor\n",
    "    processor = SegformerImageProcessor.from_pretrained(model_name)\n",
    "    try:\n",
    "        model = SegformerForSemanticSegmentation.from_pretrained(model_name)\n",
    "    except:\n",
    "        model = Mask2FormerForUniversalSegmentation.from_pretrained(model_name)\n",
    "\n",
    "    log_per_model = []\n",
    "\n",
    "    for i in range(1):  # Assuming you have 10 images numbered from 0 to 9\n",
    "        image = Image.open(image_path)  # Load an image\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.logits  # Or however you extract the logits/predictions\n",
    "        log_per_model.append(logits)\n",
    "\n",
    "    logits_list.append(log_per_model)\n",
    "\n",
    "    # show the image\n",
    "    upsampled_logits = torch.nn.functional.interpolate(\n",
    "        logits,\n",
    "        size=image.size[::-1],  # (height, width)\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "    img_seg, seg = get_seg_img(\n",
    "        image, upsampled_logits.argmax(dim=1)[0]\n",
    "    )  # Assuming get_seg_img is defined\n",
    "    plt.imshow(seg)\n",
    "    plt.axis(\"off\")  # Optional: to hide axes for a cleaner visualization\n",
    "\n",
    "# Process or save logits_list as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names2 = [\n",
    "    \"facebook/mask2former-swin-large-cityscapes-instance\",\n",
    "    \"facebook/mask2former-swin-large-ade-semantic\",\n",
    "]\n",
    "\n",
    "logits_list2 = []\n",
    "\n",
    "for model_name in model_names2:\n",
    "    print(model_name)\n",
    "    # Load the model and its preprocessor\n",
    "    processor = SegformerImageProcessor.from_pretrained(model_name)\n",
    "    try:\n",
    "        model = SegformerForSemanticSegmentation.from_pretrained(model_name)\n",
    "    except:\n",
    "        try:\n",
    "            model = Mask2FormerForUniversalSegmentation.from_pretrained(model_name)\n",
    "        except:\n",
    "            model = ViTModel.from_pretrained(model_name)\n",
    "\n",
    "    log_per_model = []\n",
    "\n",
    "    for i in range(10):  # Assuming you have 10 images numbered from 0 to 9\n",
    "        image = Image.open(image_path)  # Load an image\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        try:  # segformer\n",
    "            logits = outputs.logits  # Or however you extract the logits/predictions\n",
    "        except:  # mask2former\n",
    "            logits = outputs.masks_queries_logits\n",
    "\n",
    "        log_per_model.append(logits)\n",
    "\n",
    "    logits_list.append(log_per_model)\n",
    "\n",
    "    # show the image\n",
    "    upsampled_logits = torch.nn.functional.interpolate(\n",
    "        logits,\n",
    "        size=image.size[::-1],  # (height, width)\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "    img_seg, seg = get_seg_img(\n",
    "        image, upsampled_logits.argmax(dim=1)[0]\n",
    "    )  # Assuming get_seg_img is defined\n",
    "    plt.imshow(seg)\n",
    "    plt.axis(\"off\")  # Optional: to hide axes for a cleaner visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"image-segmentation\", model=\"facebook/maskformer-swin-base-coco\")\n",
    "\n",
    "# show the image\n",
    "image = Image.open(image_path)  # Load an image\n",
    "result = pipe(image)\n",
    "plt.imshow(result[\"segmentation\"])\n",
    "plt.axis(\"off\")  # Optional: to hide axes for a cleaner visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_short_names = [\n",
    "    \"Seg-B3-ADE\",\n",
    "    \"Seg-B1-CS\",\n",
    "    \"Seg-B3-CS\",\n",
    "    \"Seg-B5-CS\",\n",
    "    \"M2F-SL-ADE\",\n",
    "    \"M2F-SL-CS\",\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    10, len(model_short_names), figsize=(20, 27)\n",
    ")  # Adjust size as needed\n",
    "\n",
    "for img_idx in range(10):  # Assuming you have 10 images per model\n",
    "    image = Image.open(image_path)\n",
    "    for model_idx, model_name in enumerate(model_short_names):\n",
    "        upsampled_logits = nn.functional.interpolate(\n",
    "            logits_list[model_idx][\n",
    "                img_idx\n",
    "            ],  # Assuming each model has a list of logits for each image\n",
    "            size=image.size[::-1],  # Assuming 'image' is your PIL Image or similar\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "\n",
    "        # Convert upsampled logits to segmentation image\n",
    "        img_seg, _ = get_seg_img(image, upsampled_logits.argmax(dim=1)[0])\n",
    "\n",
    "        ax = axs[img_idx, model_idx]\n",
    "        ax.imshow(img_seg)\n",
    "        ax.axis(\"off\")  # Hide axes for a cleaner visualization\n",
    "\n",
    "        if model_idx == 0:\n",
    "            # Label the row with the model name\n",
    "            ax.set_ylabel(\n",
    "                f\"frame_X.jpg\", rotation=90, size=\"large\", labelpad=20\n",
    "            )\n",
    "\n",
    "        if img_idx == 0:\n",
    "            ax.set_title(model_name, size=\"large\", pad=20)\n",
    "\n",
    "\n",
    "# for ax, img_idx in zip(axs[:, 0], range(10)):\n",
    "#     ax.annotate(f'frame_24{img_idx}00', xy=(-0, 0), xytext=(-ax.yaxis.labelpad + 5, 0),\n",
    "#                  ha='right', va='center', rotation=90)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to accommodate titles\n",
    "plt.suptitle(\"Comparison Segmentation\", size=\"x-large\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def load_logits(img_shape, seg_path):\n",
    "    # Load logits from the specified path\n",
    "    logits = torch.load(seg_path)    \n",
    "\n",
    "    # Check if the shape matches the expected shape\n",
    "    if logits.shape == torch.Size([1, 19, 128, 128]):\n",
    "        logits = logits.float()  # Convert to 'float' data type\n",
    "        \n",
    "        # Upsample the logits to match the image shape\n",
    "        upsampled_logits = nn.functional.interpolate(logits, size=img_shape, mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Get both the maximum probabilities and their corresponding labels\n",
    "        max_probs, labels = torch.max(upsampled_logits, dim=1)\n",
    "        label = labels[0]  # Extract the most probable label matrix\n",
    "        prob = max_probs[0]  # Extract the probability matrix corresponding to the most probable labels\n",
    "        \n",
    "        print(\"Upsampled!\")\n",
    "        return label, prob  # Return both the label matrix and the probability matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoImageProcessor, MaskFormerForInstanceSegmentation\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/maskformer-swin-base-coco\")\n",
    "model = MaskFormerForInstanceSegmentation.from_pretrained(\"facebook/maskformer-swin-base-coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, CLIPSegForImageSegmentation\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
    "model = CLIPSegForImageSegmentation.from_pretrained(\"CIDAS/clipseg-rd64-refined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoImageProcessor, UperNetForSemanticSegmentation\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"openmmlab/upernet-swin-small\")\n",
    "model = UperNetForSemanticSegmentation.from_pretrained(\"openmmlab/upernet-swin-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, OneFormerForUniversalSegmentation\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"shi-labs/oneformer_ade20k_swin_large\")\n",
    "model = OneFormerForUniversalSegmentation.from_pretrained(\"shi-labs/oneformer_ade20k_swin_large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):  # Assuming you have 10 images numbered from 0 to 9\n",
    "    image = Image.open(\n",
    "        f\"/home/pvondrlik/Desktop/BA_Thesis/repo-movie-analysis/data/Expl_2_ET_1_2023-09-06_10-36-37_ET/video_frames_img/frame_25{i}00.jpg\"\n",
    "    )  # Load an image\n",
    "    inputs = processor(images=image, task_inputs=\"instance\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits  # Or however you extract the logits/predictions\n",
    "\n",
    "    upsampled_logits_org = nn.functional.interpolate(\n",
    "        logits,\n",
    "        size=image.size[::-1],  # (height, width)\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    img_seg, _ = get_seg_img(image, upsampled_logits_org.argmax(dim=1)[0])\n",
    "    plt.imshow(img_seg)\n",
    "    plt.show()\n",
    "\n",
    "logits_list.append(log_per_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoProcessor, CLIPSegForImageSegmentation\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
    "model = CLIPSegForImageSegmentation.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n",
    "\n",
    "prompts = [\n",
    "    \"street\",\n",
    "    \"buildings\",\n",
    "    \"people\",\n",
    "    \"living things\",\n",
    "    \"sidewalk\",\n",
    "    \"shadow\",\n",
    "    \"fence\",\n",
    "]\n",
    "\n",
    "inputs = processor(\n",
    "    text=prompts,\n",
    "    images=[image] * len(prompts),\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "# predict\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "preds = outputs.logits.unsqueeze(1)\n",
    "\n",
    "# visualize prediction\n",
    "_, ax = plt.subplots(1, 5, figsize=(15, 4))\n",
    "[a.axis(\"off\") for a in ax.flatten()]\n",
    "ax[0].imshow(image)\n",
    "[ax[i + 1].imshow(torch.sigmoid(preds[i][0])) for i in range(4)]\n",
    "[ax[i + 1].text(0, -15, prompts[i]) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoImageProcessor, UperNetForSemanticSegmentation\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"openmmlab/upernet-swin-small\")\n",
    "model = UperNetForSemanticSegmentation.from_pretrained(\"openmmlab/upernet-swin-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "print(pixel_values.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "  outputs = model(pixel_values)\n",
    "\n",
    "  \n",
    "def ade_palette():\n",
    "    \"\"\"ADE20K palette that maps each class to RGB values.\"\"\"\n",
    "    return [[120, 120, 120], [180, 120, 120], [6, 230, 230], [80, 50, 50],\n",
    "            [4, 200, 3], [120, 120, 80], [140, 140, 140], [204, 5, 255],\n",
    "            [230, 230, 230], [4, 250, 7], [224, 5, 255], [235, 255, 7],\n",
    "            [150, 5, 61], [120, 120, 70], [8, 255, 51], [255, 6, 82],\n",
    "            [143, 255, 140], [204, 255, 4], [255, 51, 7], [204, 70, 3],\n",
    "            [0, 102, 200], [61, 230, 250], [255, 6, 51], [11, 102, 255],\n",
    "            [255, 7, 71], [255, 9, 224], [9, 7, 230], [220, 220, 220],\n",
    "            [255, 9, 92], [112, 9, 255], [8, 255, 214], [7, 255, 224],\n",
    "            [255, 184, 6], [10, 255, 71], [255, 41, 10], [7, 255, 255],\n",
    "            [224, 255, 8], [102, 8, 255], [255, 61, 6], [255, 194, 7],\n",
    "            [255, 122, 8], [0, 255, 20], [255, 8, 41], [255, 5, 153],\n",
    "            [6, 51, 255], [235, 12, 255], [160, 150, 20], [0, 163, 255],\n",
    "            [140, 140, 140], [250, 10, 15], [20, 255, 0], [31, 255, 0],\n",
    "            [255, 31, 0], [255, 224, 0], [153, 255, 0], [0, 0, 255],\n",
    "            [255, 71, 0], [0, 235, 255], [0, 173, 255], [31, 0, 255],\n",
    "            [11, 200, 200], [255, 82, 0], [0, 255, 245], [0, 61, 255],\n",
    "            [0, 255, 112], [0, 255, 133], [255, 0, 0], [255, 163, 0],\n",
    "            [255, 102, 0], [194, 255, 0], [0, 143, 255], [51, 255, 0],\n",
    "            [0, 82, 255], [0, 255, 41], [0, 255, 173], [10, 0, 255],\n",
    "            [173, 255, 0], [0, 255, 153], [255, 92, 0], [255, 0, 255],\n",
    "            [255, 0, 245], [255, 0, 102], [255, 173, 0], [255, 0, 20],\n",
    "            [255, 184, 184], [0, 31, 255], [0, 255, 61], [0, 71, 255],\n",
    "            [255, 0, 204], [0, 255, 194], [0, 255, 82], [0, 10, 255],\n",
    "            [0, 112, 255], [51, 0, 255], [0, 194, 255], [0, 122, 255],\n",
    "            [0, 255, 163], [255, 153, 0], [0, 255, 10], [255, 112, 0],\n",
    "            [143, 255, 0], [82, 0, 255], [163, 255, 0], [255, 235, 0],\n",
    "            [8, 184, 170], [133, 0, 255], [0, 255, 92], [184, 0, 255],\n",
    "            [255, 0, 31], [0, 184, 255], [0, 214, 255], [255, 0, 112],\n",
    "            [92, 255, 0], [0, 224, 255], [112, 224, 255], [70, 184, 160],\n",
    "            [163, 0, 255], [153, 0, 255], [71, 255, 0], [255, 0, 163],\n",
    "            [255, 204, 0], [255, 0, 143], [0, 255, 235], [133, 255, 0],\n",
    "            [255, 0, 235], [245, 0, 255], [255, 0, 122], [255, 245, 0],\n",
    "            [10, 190, 212], [214, 255, 0], [0, 204, 255], [20, 0, 255],\n",
    "            [255, 255, 0], [0, 153, 255], [0, 41, 255], [0, 255, 204],\n",
    "            [41, 0, 255], [41, 255, 0], [173, 0, 255], [0, 245, 255],\n",
    "            [71, 0, 255], [122, 0, 255], [0, 255, 184], [0, 92, 255],\n",
    "            [184, 255, 0], [0, 133, 255], [255, 214, 0], [25, 194, 194],\n",
    "            [102, 255, 0], [92, 0, 255]]\n",
    "\n",
    "seg = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n",
    "color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "palette = np.array(ade_palette())\n",
    "for label, color in enumerate(palette):\n",
    "    color_seg[seg == label, :] = color\n",
    "# Convert to BGR\n",
    "color_seg = color_seg[..., ::-1]\n",
    "\n",
    "# Show image + mask\n",
    "img = np.array(image) * 0.5 + color_seg * 0.5\n",
    "img = img.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, OneFormerForUniversalSegmentation\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"shi-labs/oneformer_ade20k_swin_large\")\n",
    "model = OneFormerForUniversalSegmentation.from_pretrained(\n",
    "    \"shi-labs/oneformer_ade20k_swin_large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from super_gradients.training import models\n",
    "from ultralytics import YOLO\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load a YOLO model\n",
    "model = YOLO(\"yolov8n.pt\")  # Or your custom model\n",
    "\n",
    "# Run prediction\n",
    "results1 = model(\n",
    "    path,\n",
    ")  # Replace with your source\n",
    "\n",
    "# Convert results image to PIL Image for Streamlit\n",
    "annotated_img = Image.fromarray(results[0].plot()[..., ::-1])  # Convert BGR to RGB\n",
    "\n",
    "# Display the image in Streamlit\n",
    "st.image(annotated_img, caption=\"Annotated Image\")\n",
    "\n",
    "yolo_nas_l = models.get(\"yolo_nas_l\", pretrained_weights=\"coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_result_pyplot(model, img, result, score_thr=0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ba-cv-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
